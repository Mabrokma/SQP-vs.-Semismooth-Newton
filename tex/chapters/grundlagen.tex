\chapter{Einführung}

Die nichtlineare Optimierung ist ein bedeutendes Gebiet der Mathematik.
Sie findet immer wieder Anwendungen in der schwierigen Problemen der Technik und
der Wirtschaft. Es wurden viele Verfahren entwickelt, um nichtlineare
Optimierungsprobleme zu lösen. In dieser Arbeit werden zwei Verfahren, das
halbglatte Newton-Verfahren und das SQP-Verfahren, betrachtet und verglichen.

Das SQP-Verfahren gehört zu den bekanntesten Verfahren der nichtlinearen
Optimierung. Es wurde schon seit den 60er Jahren entwickelt und wurde in vielen
Optimierungsproblemen angewendet sowie weiterentwickelt. Das halbglatte
Newton-Verfahren ist weniger bekannt als das SQP-Verfahren. Es basiert aber auf
das bekannte Newton-Verfahren.

\section{Optimierungsprobleme}

Allgemein ist die Aufgabenstellung der nichtlinearen Optimierung wie folgt
definiert:
\begin{equation}
  \min_{x \in \F} f(x)
\end{equation}

Die Funktion $f:\R^n \rightarrow \R$ ist die sogennante Zielfunktion.
$\F$ ist eine Teilmenge von $\R^n$, die man als Lösungsmenge bezeichnet.
Alle Elemente von $\F$ werden als zulässige Punkte bezeichnet.
$\F$ wird oft durch Nebenbedingungen definiert.

Man kann hierbei den Unterschied zwischen der linearen Optimierung und der
nichtlinearen Optimierung erkennen. Bei der linearen Optimierung muss die
Zielfunktion linear sein und die Nebenbedingungen sind durch lineare
Gleichungssysteme oder Ungleichungssyteme definiert. Bei der nichtlinearen
Optimierung gibt es dagegen keine Einschränkung, wie die Zielfunktion und die
Nebenbedingungen aussehen sollen.
Für die lineare Optimierung ist ein in der Praxis sehr effizientes Verfahren
bekannt. Man kann aber auch die Verfahren der nichtlinearen Optimierung für die
linearen Optimierungsprobleme anwenden. Umgekehrt ist das aber nicht möglich.

Ein einfaches Beispiel nichtlinearer Optimierung ist das Problem
\[
  \min_{x \in \R} (x-1)^2.
\]

Falls $\F = \R^n$ gilt, bezeichnet man das Optimierungsproblem als
unrestringiert. Es besitzt also keine Nebenbedingungen. Ansonsten heißt es ein
restingiertes Optimierungsproblem.

% TODO: Minima?

% TODO: Globale Optimierung vs. Lokale Optimierung

% TODO?: Konvexität

% TODO: Iterative Methode: \[ f(x^{k+1}) < f(x^k) \]

% TODO: Unrestringerte Optimierungsprobleme: Theorie und Verfahren

% TODO: Approximation der Gradient und Hesse-Matrix?
